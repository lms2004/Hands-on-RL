import copy


class CliffWalkingEnv:
    """ æ‚¬å´–æ¼«æ­¥ç¯å¢ƒ"""
    def __init__(self, ncol=12, nrow=4):
        self.ncol = ncol  # å®šä¹‰ç½‘æ ¼ä¸–ç•Œçš„åˆ—
        self.nrow = nrow  # å®šä¹‰ç½‘æ ¼ä¸–ç•Œçš„è¡Œ
        # è½¬ç§»çŸ©é˜µP[state][action] = [(p, next_state, reward, done)]åŒ…å«ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±
        self.P = self.createP()

    def createP(self):
        # åˆå§‹åŒ–
        P = [[[] for j in range(4)] for i in range(self.nrow * self.ncol)]
        # 4ç§åŠ¨ä½œ, change[0]:ä¸Š,change[1]:ä¸‹, change[2]:å·¦, change[3]:å³ã€‚åæ ‡ç³»åŸç‚¹(0,0)
        # å®šä¹‰åœ¨å·¦ä¸Šè§’
        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]
        for i in range(self.nrow):
            for j in range(self.ncol):
                for a in range(4):
                    # ä½ç½®åœ¨æ‚¬å´–æˆ–è€…ç›®æ ‡çŠ¶æ€,å› ä¸ºæ— æ³•ç»§ç»­äº¤äº’,ä»»ä½•åŠ¨ä½œå¥–åŠ±éƒ½ä¸º0
                    if i == self.nrow - 1 and j > 0:
                        P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0,
                                                    True)]
                        continue
                    # å…¶ä»–ä½ç½®
                    next_x = min(self.ncol - 1, max(0, j + change[a][0]))
                    next_y = min(self.nrow - 1, max(0, i + change[a][1]))
                    next_state = next_y * self.ncol + next_x
                    reward = -1
                    done = False
                    # ä¸‹ä¸€ä¸ªä½ç½®åœ¨æ‚¬å´–æˆ–è€…ç»ˆç‚¹
                    if next_y == self.nrow - 1 and next_x > 0:
                        done = True
                        if next_x != self.ncol - 1:  # ä¸‹ä¸€ä¸ªä½ç½®åœ¨æ‚¬å´–
                            reward = -100
                    P[i * self.ncol + j][a] = [(1, next_state, reward, done)]
        return P

class PolicyIteration:
    """ ç­–ç•¥è¿­ä»£ç®—æ³•ï¼ˆPolicy Iterationï¼‰ """

    def __init__(self, env, theta, gamma):
        self.env = env
        self.v = [0] * (self.env.ncol * self.env.nrow)  # åˆå§‹åŒ– V(s)=0
        self.pi = [[0.25, 0.25, 0.25, 0.25]              # åˆå§‹åŒ–ç­–ç•¥ä¸ºå‡åŒ€åˆ†å¸ƒï¼šÏ€(a|s)=0.25
                   for _ in range(self.env.ncol * self.env.nrow)]
        self.theta = theta  # ä»·å€¼å‡½æ•°æ”¶æ•›é˜ˆå€¼ Îµ
        self.gamma = gamma  # æŠ˜æ‰£å› å­ Î³ âˆˆ [0, 1]

    def policy_evaluation(self):  # ç­–ç•¥è¯„ä¼° (Policy Evaluation)
        cnt = 1  # è®°å½•è¿­ä»£è½®æ•°ï¼ˆä»…ç”¨äºæ‰“å°æ—¥å¿—ï¼‰
        
        while True: # æ¯æ¬¡è¿­ä»£ V â† V_k+1
            max_diff = 0  # å½“å‰è½®ä¸­æ‰€æœ‰çŠ¶æ€ä»·å€¼å˜åŠ¨çš„æœ€å¤§å€¼ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦æ”¶æ•›ï¼‰
            
            # åˆå§‹åŒ–ä¸€ä¸ªæ–°ä¸€è½®çš„çŠ¶æ€ä»·å€¼å‡½æ•° V_k+1(s)ï¼Œåˆå§‹ä¸º0ï¼ˆç¨åä¼šå¡«å…¥æ–°è®¡ç®—çš„å€¼ï¼‰
            new_v = [0] * (self.env.ncol * self.env.nrow)

            # éå†æ‰€æœ‰çŠ¶æ€ s âˆˆ ğ’®
            for s in range(self.env.ncol * self.env.nrow):
                qsa_list = []  # ç”¨äºå­˜å‚¨å½“å‰çŠ¶æ€ s ä¸‹ï¼Œæ‰€æœ‰åŠ¨ä½œçš„ Ï€(a|s) * Q(s,a)
                
                # éå†å½“å‰çŠ¶æ€ä¸‹çš„æ‰€æœ‰å¯èƒ½åŠ¨ä½œ a âˆˆ ğ’œï¼ˆæ­¤å¤„åŠ¨ä½œæ•°ä¸º4ï¼‰
                for a in range(4):
                    qsa = 0  # QÏ€(s,a) ç´¯åŠ å™¨

                    # éå†æ‰€æœ‰ (p, s', r, done)ï¼šåœ¨çŠ¶æ€ s æ‰§è¡ŒåŠ¨ä½œ a åå¯èƒ½åˆ°è¾¾çš„åç»§çŠ¶æ€ s'
                    for p, next_state, r, done in self.env.P[s][a]:
                        # p: è½¬ç§»æ¦‚ç‡ P(s'|s,a)
                        # next_state: ä¸‹ä¸€çŠ¶æ€ s'
                        # r: å³æ—¶å¥–åŠ± r(s,a,s')
                        # done: æ˜¯å¦æ˜¯ç»ˆæ­¢çŠ¶æ€

                        # QÏ€(s,a) += P(s'|s,a) * [r + Î³ * V_k(s')]
                        # å¦‚æœæ˜¯ç»ˆæ­¢çŠ¶æ€(done=True)ï¼Œåˆ™ä¸è€ƒè™‘åç»­çŠ¶æ€ä»·å€¼
                        qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))
                    
                    # Ï€(a|s) * QÏ€(s,a)
                    qsa_list.append(self.pi[s][a] * qsa)
                
                # VÏ€(s) = âˆ‘_{a} Ï€(a|s) * QÏ€(s,a)ï¼Œç­–ç•¥ä¸‹çš„çŠ¶æ€ä»·å€¼å‡½æ•°
                new_v[s] = sum(qsa_list)

                # è®°å½•æ–°æ—§ä»·å€¼å‡½æ•°çš„æœ€å¤§å·®å¼‚ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦æ”¶æ•›
                max_diff = max(max_diff, abs(new_v[s] - self.v[s]))

            # æ›´æ–°å½“å‰çš„çŠ¶æ€ä»·å€¼å‡½æ•° V â† V_k+1
            self.v = new_v

            # è‹¥æ‰€æœ‰çŠ¶æ€çš„æ›´æ–°å€¼éƒ½å°äºé˜ˆå€¼ Î¸ï¼Œè¯´æ˜å·²æ”¶æ•›ï¼Œé€€å‡ºå¾ªç¯
            if max_diff < self.theta:
                break

            cnt += 1  # å¦åˆ™ç»§ç»­è¿­ä»£

        print("ç­–ç•¥è¯„ä¼°è¿›è¡Œ %d è½®åå®Œæˆ" % cnt)


    def policy_improvement(self):  # ç­–ç•¥æå‡ (Policy Improvement)
        for s in range(self.env.nrow * self.env.ncol):
            qsa_list = []
            for a in range(4):
                qsa = 0
                for p, next_state, r, done in self.env.P[s][a]:
                    # QÏ€(s, a) = âˆ‘_{s'} P(s'|s,a) * [r(s,a) + Î³ V(s')]
                    qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))
                qsa_list.append(qsa)
            maxq = max(qsa_list)
            cntq = qsa_list.count(maxq)
            # Ï€'(a|s) = 1/cntq if Q(a)=maxQ else 0 ï¼ˆå‡åˆ†æœ€ä¼˜åŠ¨ä½œï¼‰
            self.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list]
        print("ç­–ç•¥æå‡å®Œæˆ")
        return self.pi

    def policy_iteration(self):  # ç­–ç•¥è¿­ä»£ä¸»å‡½æ•°
        while True:
            self.policy_evaluation()  # ä½¿ç”¨ Ï€ è¯„ä¼° VÏ€
            old_pi = copy.deepcopy(self.pi)
            new_pi = self.policy_improvement()  # ä½¿ç”¨ VÏ€ æå‡ Ï€
            if old_pi == new_pi:  # è‹¥ç­–ç•¥ä¸å†å˜åŒ– â†’ Ï€ å·²æœ€ä¼˜
                break

def print_agent(agent, action_meaning, disaster=[], end=[]):
    print("çŠ¶æ€ä»·å€¼ï¼š")
    for i in range(agent.env.nrow):
        for j in range(agent.env.ncol):
            # ä¸ºäº†è¾“å‡ºç¾è§‚,ä¿æŒè¾“å‡º6ä¸ªå­—ç¬¦
            print('%6.6s' % ('%.3f' % agent.v[i * agent.env.ncol + j]),
                  end=' ')
        print()

    print("ç­–ç•¥ï¼š")
    for i in range(agent.env.nrow):
        for j in range(agent.env.ncol):
            # ä¸€äº›ç‰¹æ®Šçš„çŠ¶æ€,ä¾‹å¦‚æ‚¬å´–æ¼«æ­¥ä¸­çš„æ‚¬å´–
            if (i * agent.env.ncol + j) in disaster:
                print('****', end=' ')
            elif (i * agent.env.ncol + j) in end:  # ç›®æ ‡çŠ¶æ€
                print('EEEE', end=' ')
            else:
                a = agent.pi[i * agent.env.ncol + j]
                pi_str = ''
                for k in range(len(action_meaning)):
                    pi_str += action_meaning[k] if a[k] > 0 else 'o'
                print(pi_str, end=' ')
        print()


env = CliffWalkingEnv()
action_meaning = ['^', 'v', '<', '>']
theta = 0.001 # å€¼å‡½æ•°æ”¶æ•›çš„è¯¯å·®é˜ˆå€¼
gamma = 0.9 # æŠ˜æ‰£å› å­
agent = PolicyIteration(env, theta, gamma)
agent.policy_iteration()
print_agent(agent, action_meaning, list(range(37, 47)), [47])
